---
title: floating point
author: eranbu
date: 2.2026
marp: true
theme: gaia
paginate: true
math: mathjax
---

![bg left width:380px](images/float.png)

# Floating Point

## ğŸŒŠ Fluid Numbers in C++

<!-- Intro to floats. -->

---

# Agenda

1. ğŸ’¾ Representation (IEEE 754)
2. ğŸ¯ Accuracy & Math Rules
3. ğŸ”„ Binary Consistency
4. ğŸš€ Hardware & Optimization (SIMD, ARM, Bottlenecks)

<!-- The plan. -->

---

# ğŸ’¾ Representation

![bg right width:600px](images/ieee754.png)


---

<!-- _class: lead -->

# ğŸ§© Puzzle: Arithmetic

```cpp
if (0.1 + 0.2 == 0.3) {
    std::cout << "Equal!";
} else {
    std::cout << "Not Equal!";
}
```

* A) Equal
* B) Not Equal
* C) Compilation Error
* D) Not Equal with -O3

<!-- Answer: B. 0.1 and 0.2 are infinite fractions in binary. sum is slightly larger than 0.3. -->

---

# ğŸ§© Puzzle: Order Matters?

```cpp
float a = 1.0f;
float b = 1e20f;
float c = -1e20f;

// Algebra: (a + b) + c  ==  a + (b + c)
std::cout << (a + b) + c << "\n";
std::cout << a + (b + c) << "\n";
```

* A) `1`, `1`
* B) `0`, `1`
* C) `1`, `0`
* D) `0`, `0`

<!-- Answer: B. (1 + 1e20) is 1e20 (percentage of 1 is too small). 1e20 - 1e20 = 0.
1 + (1e20 - 1e20) = 1 + 0 = 1. Associativity holds for Reals, NOT Floats! -->

---

# ğŸ§© Puzzle: Division vs Reciprocal

```cpp
float a = 1.2f;
float b = a / 3.0f;
float c = a * (1.0f / 3.0f);

if (b == c) {
    std::cout << "Equal";
} else {
    std::cout << "Not Equal";
}
```

* A) Equal
* B) Not Equal
* C) Depends on Flags
* D) Compilation Error

<!-- Answer: B (Usually). Division is NOT multiplication by reciprocal (rounding differences). 
However, C is also correct if `-freciprocal-math` is used! -->

---


## ğŸ’¾ IEEE 754

* ğŸ”¢ Sign bit, Exponent, Mantissa.
* ğŸ“ `float` (32-bit), `double` (64-bit).
* â™¾ï¸ Special values: `Inf`, `-Inf`, `NaN`.

<!-- IEEE 754 details. S E M. -->

---

## ğŸ¯ Accuracy

* âŒ Not all real numbers typically represented exactly.
* ğŸ§® `0.1 + 0.2 != 0.3`

<!-- 0.1 + 0.2 != 0.3 demo. -->

---

## ğŸ“ Math Rules

* âš ï¸ Associativity holds in algebra, NOT in floating point.
* ğŸ”¢ `(a + b) + c != a + (b + c)`
* ğŸš© **Implication**: Compilation flags can change results (reordering).

<!-- Associativity fails. Compiler reordering. -->

---

# ğŸŒ Example: Geo Coordinates

* Earth Radius $R \approx 6,371,000$ meters.
* `float` (23 bits) has $\approx 7$ significant digits.
* Precision at $R$:
    * $6.371 \times 10^6 \times 1.19 \times 10^{-7} \approx 0.75$ meters.
* âš ï¸ **1 Meter Error!**
* **Conclusion**: Use `double` for ECEF / UTM coordinates!

<!-- Show code example of nextafter. -->

---

# ğŸ§© Puzzle: NaNs
```cpp
using n = std::numeric_limits<double>;
auto q = n::quiet_NaN();
auto s = n::signaling_NaN();
double r1 = q + 1.0; 
double r2 = s + 1.0;
```
*Which operation crash ?*
* A) None (Both silent)
* B) 1 only
* C) 2 only
* D) Both

<!-- Answer: A. qNaN propagates silently. sNaN signals invalid operation. 
BUT...  Environment must support/enable traps for it to crash, but the flag is raised. -->

---

# âš ï¸ Signaling vs Quiet NaN

* **qNaN (Quiet NaN)**:
    * Default `NaN`. Propagates silently.
    * **Set**: `std::numeric_limits<T>::quiet_NaN()` or `0.0/0.0`.
    * **Check**: `std::isnan(x)`.
* **sNaN (Signaling NaN)**:
    * Triggers exception (`FE_INVALID`) on use. Rare.
    * **Set**: `std::numeric_limits<T>::signaling_NaN()`.

<!-- 
sNaN traps. qNaN flows. 
Note: Math operations (0/0, sqrt(-1)) produce qNaN. 
sNaN is manually created (numeric_limits) or from uninitialized memory (if supported). 
-->

---

# ğŸ§© Puzzle: Manual Abs?

```cpp
float my_abs(float f) {
    int* i = (int*)&f;
    *i &= 0x7FFFFFFF; // Clear sign bit
    return f;
}
```
*What happens?*

* A) Returns `abs(f)`
* B) Undefined Behavior
* C) Returns `-f`
* D) Compilation Error

<!-- Answer: A. But .. Strict Aliasing Violation! . -->

---

# ğŸ“‰ Denormals (Subnormals)

* **What?** Very small numbers (near 0) without leading 1.
    * $0.mantissa \times 2^{min\_exp}$ vs $1.mantissa \times 2^{exp}$
* **Problem**: handled by Microcode / Software Trap ğŸ¢.
    * Can be **100x slower**!
* **Solution**:
    * **FTZ** (Flush-to-Zero): Output $\to$ 0.
    * **DAZ** (Denormals-Are-Zero): Input $\to$ 0.
    * Standard in Audio / Games.

<!-- Denormals are slow. Use FTZ/DAZ. -->

---



<!-- _class: lead -->

# ğŸ”„ Binary Consistency & Environment

![bg right width:300px](images/consistency.png)

---

## ğŸ”„ Consistency (Why results differ?)

* ğŸ’» **Same code, different results?** Yes!
* âš™ï¸ **Instruction Sets (ISA)**:
    * **FMA** (`a*b+c`): One step, **one rounding**.
    * **No FMA**: `(a*b)+c`: Two steps, **two roundings**.
* ğŸ—ï¸ **Architecture & Dispatching**:
    * **AVX / AVX2 / AVX-512**: Different instructions -> different rounding.
    * **Libraries (IPP/MKL)**: **Auto-dispatch** code based on CPU.
    * âš ï¸ **Result**: Same binary gives different results on i7 vs i9!
* ğŸ§µ **Parallelism**:
    * Reduction `sum(vector)` depends on order (threads/chunks).
    * Since `(a+b)+c != a+(b+c)`, order changes result!

<!-- Different results on different machines? Yes. -->

---

## ğŸš© Flags and Optimization

* ğŸš€ `-ffast-math` (or `-Ofast`):
    * `-fno-math-errno`, `-funsafe-math-optimizations`
    * `-ffinite-math-only`, `-fno-rounding-math`
    * `-fno-signaling-nans`, `-fcx-limited-range`
    * `-fexcess-precision=fast`
* âš ï¸ **Unsafe Math Includes**:
    * `-fno-signed-zeros`, `-fno-trapping-math`
    * `-fassociative-math`, `-freciprocal-math`

<!--
-ffast-math breakdown:

* -fno-math-errno: `sqrt(-1)` returns NaN but doesn't set global `errno`. (Syscall avoidance).
* -funsafe-math-optimizations: Enables the risky ones below.
* -ffinite-math-only: Assumes NO NaN/Inf! `isnan(x)` -> `false` (always). `x != x` -> `false`.
* -fno-rounding-math: Assumes default (Nearest) rounding. Changing `fesetround` won't affect optimizers (constant folding).
* -fno-signaling-nans: sNaN behaves like qNaN (no trap).
* -fcx-limited-range: Complex mult `(a+bi)(c+di)` uses simple formula `ac-bd...`. Overflows easier.
* -fexcess-precision=fast: Casts to `float`/`double` immediately (no 80-bit temp precision on x87).

Unsafe/optimization flags:
* -fno-signed-zeros: Treat `-0.0` as `+0.0`. `x + 0.0 -> x` (safe?).
* -fno-trapping-math: Assume code doesn't trap. Moves instructions past checks.
* -fassociative-math: `(a + b) + c` == `a + (b + c)`. Enables vectorization/parallel reduction!
* -freciprocal-math: `x / y` -> `x * (1/y)`. `1/y` computed once. 1.0/3.0 is not exactly 1/3.
-->

---

## âš¡ Intel IPP / MKL

* ğŸï¸ **Hand-Tuned Assembly**: Beats compiler auto-vectorization.
* ğŸ”„ **Auto-Dispatch (Merged Static)**:
    * Single binary contains code for SSE, AVX, AVX-512.
    * `ippInit()` detects CPU and points to correct function.
* ğŸ¯ **Single Processor Static Linkage**:
    * Link **only** the AVX2 code (e.g., `ipp_h9.lib` / custom build).
    * âœ… **Smallest binary**. No runtime dispatch overhead.
    * âš ï¸ **Risk**: Crashes if run on unsupported CPU (e.g., old Atom).
    * *Use case*: Embedded systems, known hardware.

---


# ğŸ“ Summary

* ğŸ¤·â€â™‚ï¸ Reproducibility is hard with floats.

<!-- Reproducibility issues. -->

---

<!-- _class: lead -->

# ğŸš€ Hardware Aspects

![bg right width:300px](images/cpu.png)

---

## âš¡ Intel IPP / MKL

* ğŸï¸ **Hand-Tuned Assembly**: Beats compiler auto-vectorization.
* ğŸ”„ **Auto-Dispatch (Merged Static)**:
    * Single binary contains code for SSE, AVX, AVX-512.
    * `ippInit()` detects CPU and points to correct function.
* ğŸ¯ **Single Processor Static Linkage**:
    * Link **only** the AVX2 code (e.g., `ipp_h9.lib` / custom build).
    * âœ… **Smallest binary**. No runtime dispatch overhead.
    * âš ï¸ **Risk**: Crashes if run on unsupported CPU (e.g., old Atom).
    * *Use case*: Embedded systems, known hardware.

<!-- IPP Static Linking. Dispatch vs Single Processor. -->

---

## ğŸ›ï¸ Floating Point Control Register

* **MXCSR** (SSE) / (**FPU CW** (x87)).
* Controls:
    * **Rounding Mode** (Nearest, Down, Up, Zero).
    * **Exception Masks** (Invalid, DivByZero, etc.).
* âš ï¸ **Global State!**

---

## ğŸ’¥ Example: Rounding Mode Bleed

```cpp
void bad_lib() {
    std::fesetround(FE_DOWNWARD); // Global change!
}

int main() {
    // 1.0 / 3.0 normally 0.33333334
    bad_lib();
    // Now it might be 0.33333331!
}
```

<!-- Rounding mode affects everything. Be careful with libraries. -->
<!-- Full example: numbers/examples/rounding/main.cpp -->

---

## ğŸ“± ARM / Neon

* ï¿½ï¸ **FMA (Fused Multiply Add)**: Standard in ARMv8 (AArch64).
    * x86 only got FMA recently (Haswell). **Results will differ!**
* ğŸ›‘ **Denormals / FTZ**:
    * Older NEON (32-bit) **always** flushed to zero (Non-IEEE).
    * AArch64 is IEEE compliant but often runs in FTZ mode for speed.
* âš ï¸ **NaN Propagation**:
    * Default NaN payloads may differ from x86.
* **Exceptions**: Neon often doesn't generate FP traps by default.

<!-- Neon, FTZ. FMA is standard. -->

---

## â³ Data vs Commands Bottleneck

* ğŸŒ FP operations can be fast, waiting for data (memory) is often the bottleneck.

<!-- Memory bandwidth vs Compute. -->

---



# ğŸ“ Summary

* ğŸ’» Hardware details matter for high performance.

<!-- Hardware details. -->

---

<!-- _class: lead -->

# ğŸ›‘ Don't

ğŸš« Do not compare floating point numbers with `==`.
âœ… Use an epsilon or helper functions.

<!-- Don't use ==. -->

---

<!-- _class: lead -->

# ğŸ› ï¸ Tool

**Godbolt** (Compiler Explorer)
ğŸ”¬ Check how your floating point code compiles with different flags (`-O3`, `-ffast-math`).

<!-- Godbolt demo. -->

---

# ğŸ“ Lecture Summary

* ğŸ’¾ IEEE 754 Representation.
* ğŸ¯ Precision pitfalls and non-associativity.
* ğŸ’» Hardware influence (SIMD, Registers).
* ğŸš€ Optimization flags trade accuracy for speed.

<!-- Wrap up. -->
